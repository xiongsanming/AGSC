CLIP_MODEL_NAME_DEFAULT = "openai/clip-vit-large-patch14"
CLIP_INPUT_RESOLUTION = 224

NUM_INTERMEDIATE_LAYERS_DEFAULT = 24
PROJECTION_DIM_DEFAULT = 1024
NUM_PROJECTION_LAYERS_DEFAULT = 2
DROPOUT_RATE_DEFAULT = 0.5

ATTENTION_CROP_OUTPUT_SIZE_DEFAULT = 224
ATTN_CROP_K_TOP_PATCHES_DEFAULT = 6
ATTN_CROP_NUM_EARLY_LAYERS_TO_AGGREGATE_DEFAULT = 10
ATTN_CROP_NUM_HIGHLIGHT_LAYERS_DEFAULT = 10
ATTN_CROP_NUM_TOP_CENTROID_PATCHES_DEFAULT = 3